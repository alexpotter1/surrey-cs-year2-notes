# Parallel Computing (COM2039)

Note: This document has inline LaTeX, which won't display on GitHub. Install Typora to view this content.

### META: Add weeks 1-4

### Evaluating Parallel Algorithms

* Decompose the problem into tasks that can be executed concurrently
* The tasks may be of same, different or even indeterminate sizes
* A decomposition can be illustrated in the form of a directed graph with nodes corresponding from tasks and edges indicating that one task is a dependency for the next.
  * Known as a **task dependency graph**
  * A directed path represents a sequence of tasks that must be processed one after another
  * Longest directed path: determinant of shortest time in which the program can be executed in parallel.
* Example: **Multiplying a dense matrix with a vector**
  * No single task needs to wait for the completion of any other task
  * Tasks do share data but each task has exclusive control over separate, distinct regions
* Example 2: **Database Query Processing**
  * Consider `MODEL= "CIVIC" AND YEAR= 2001 AND (COLOR= "GREEN" OR COLOR= "WHITE")`
  * Serially, this would be quite slow
  * Can break up the database into **sub-tables**
    * Getting all Civics on thread 1
    * Getting all cars in 2001 on thread 2
    * Getting all green cars on thread 3
    * Getting all white cars on thread 4
  * Can combine results of threads (1,2) and (3,4) into two tables via `JOIN`
    * A further `JOIN` operation results in the desired output
  * This approach is much faster, and is built up in a way that **minimises task dependency**.
* Number of tasks into which a problem can be decomposed determines its **granularity**.
  * Decomposition into a large number of tasks results in fine-grained decomposition
  * Decomposition into a small number of tasks results in coarse-grained decomposition
  * Sometimes, differing levels of granularity result in faster execution times.
  * However, there is an **inherent upper bound** on how fine the granularity can be
    * For the matrix-vector multiplication example, this upper bound is no more than $n^2$ concurrent tasks
    * Tasks may need to communicate with one another, or access shared regions of memory. Here, software-based locks such as *mutexes* and *semaphores* may need to be used to ensure memory is not written to at the same time by multiple threads, *thus corrupting it*
* Number of tasks that can be executed in parallel: **degree of concurrency** (of a decomposition)
  * There is a *maximum* degree of concurrency: maximum number of tasks at a time $t'$ during execution.
  * Similarly, there's an *average* degree of concurrency: average number of tasks processed *in parallel* over time $t_{end} - t_{start}$
  *  The degree of concurrency **increases** as decomposition becomes finer in granularity, and **decreases** as decomposition becomes more coarse in granularity.
* In general, the number of tasks in a decomposition **exceeds** the number of processing elements available.
  * A parallel algorithm must also provide a mapping of tasks to processes.
    * Map tasks onto processes so that the operating system can manage the allocation of software processes to raw processing cores

#### Decomposition techniques

* Recursive decomposition
  * Can be applied to *quicksort*, *divide-and-conquer algorithms*
* Data decomposition
* Exploratory decomposition
  * Can be applied to *discrete optimisation problems*, *theorem proving*, *game playing*
* Speculative decomposition



### GPU parallelisation algorithms

#### Reduce

* Have a set of elements $\{a_1, a_2, a_3, a_4,…,a_n\}$
  * Want to reduce them to a single value $a = a_1 \oplus a_2 \oplus a_3 \oplus … \oplus a_n$
* $\oplus$ must be:
  * Binary
  * Associative

```c
int reduce(int* elements) {
    int sum = 0;
    for (int i = 0; i < sizeof(elements); i++) {
        sum = sum + elements[i];
    }
    return sum;
}
```

* Each `sum` addition is dependent on the result of the previous one
* This takes $n-1$ operations for $n$ elements, and its work complexity is $O(n)$

##### Parallelised reduce

* $((a + b) + c) + d$
  * Equivalent to $(a+b) + (c+d)$
    * $(a+b)$ on thread 0
    * $(c+d)$ on thread 1
    * Combine the results
* Step complexity: $log_2(n)$
  * Much faster than *non-parallelised* reduce

#### Scan

* Can be used to calculate partial sums of numbers

* e.g. Input = $\{1, 2, 3, 4, 5\}$

  * Output = $\{1, 3, 6, 10, 15\}$
    * First number is copied
    * Second number is $first + second$
    * Third number is $first + second + third$
    * and so on...

* Serial scan:

  * ```c
    int acc = identity;
    for (int i = 0; i < sizeof(elements); i++) {
        acc = acc op elements[i];
        out[i] = acc;
    }
    ```

* **Exclusive vs Inclusive Scan**

  * Exclusive: Starts with the identity element $I$ being used, then the first element is copied and so on...
  * Inclusive: Identity element $I$ **not** included, steps same as the first description *above* of *Scan*

* ##### Inclusive Scan: Hillis and Steele

  * Is **step-optimal**, but not necessarily **work-optimal**
  * At each step, every thread must perform a test to see if the current element has a neighbour an appropriate number of steps away
    * If it has, add that neighbour to its element.
    * If it doesn't, simply copy its value into a buffer and then out again once all the threads are synchronised

* **Inclusive Scan: Blelloch**

  * Is **work-optimal**, but not necessarily **step-optimal**
  * First - Reduce: add the numbers in the input array in the same way as a conventional reduction.
    * The difference here is that we retain the intermediate results.
  * Second - Downsweep: Replace the rightmost element with the identity for the operator we are using (0 in the case of multiplication)
    * Works in pairs
    * Add the left element in the pair to the right element
    * Then, replace the value of the left element with the value of the right element
    * The neighbour from the left is obtained from the intermediate results of the *Reduce* step
    * If we took $m = log(n)$ steps in the *Reduce* step, then in the first step of *Downsweep* we look for a neighbour $2^{m-1}$ positions to the left.
    * In the next step, look $2^{m-2}$ positions to the left, and continue on as-is
    * The downsweep should terminate after $m = log(n)$ steps.

