# Information Retrieval
## Introduction

Information Retrieval, or 'IR' for short, refers to obtaining information resources that are relevant and specific to an information requirement from a collection of sources. Searches for relevant information can be based on full-text searching, or other content-based indexing. In itself, information retrieval is a science; it is an umbrella with regards to many different methods of searching for information in a document, *sets* of documents, and even associated metadata that are linked to documents.

* Web search engines are the most commonly identifiable applications that utilise IR.

### Why is this useful?
Information Retrieval allows for systemic categorisation of resources, and for efficient categorisation of data and documents.

*Peter Morville* defined the term "Findability" in 2005 to mean:
> The ability to identify an appropriate Web site and navigate the pages of the site to discover and retrieve relevant information resources.

Findability falls under the domain of **search engine optimisation (SEO)**, **internet marketing** and **data analytics**.

There is simply too much information in existence, especially on the Internet, to be able to process in an efficient manner. In addition, with respect to social media and inter-connectivity with online services, the rate of data generation has increased exponentially over the last 10 years.

* 90% of the world's data has been generated over the last two years.

* The volume of data generated by Google's self-autonomous car is estimated to be `1GB/s` per vehicle per second.

* Eric Schmidt, of Google, Inc, said this in 2010:
  > The estimated size of all information created from the dawn of civilisation to 2003 is around 5 exabytes. At Google, we process that volume of data every 2-3 days.

 *NB: 1 exabyte = 1 million terabytes*

So, it's obvious that there needs to be huge improvements in not only network transmission capabilities, but storage and retrieval systems to be able to manage this data appropriately.

### The searchable web
Search engines, such as Google, DuckDuckGo and Bing, only cover a small proportion of the web.

* The **deep web** refers to information that is not directly indexable by web crawlers. For example, this may be private data beind a password-protected portal.
* The **deep web** is estimated to be around 400-500x the size of the **searchable web**.
* Google's page index volume has not changed significantly in recent years (~25bn pages), despite exponential growth in data generation
* Google biases results in terms of search order based on **Google PageRank**, their primary searching and categorisation algorithm, as well as website reputation and advertising campaigns.
* Different search engines will return different results, depending on the data stored in their data warehouses and specific algorithms that they use to return results.
* Page ranking can vary due to **on-page**, and **off-page** link factors.

Search algorithms and techniques do not work most efficiently at all levels of scale!
* Some are better suited to small datasets, others are suited to large datasets.

### Key metrics of Big Data
These are otherwise known as the **_4 V's of Big Data_**:
* **Volume**: Scale of data that is being manipulated.
* **Variety**: The different forms of data being manipulated.
* **Velocity**: How fast streaming data enters the system
* **Veracity**: How uncertain and unpredictable the data is

### Types of data order
This module focuses on text-based searching and retrieval of data using alphanumeric tokens. This is relatively easy, compared to image analysis (edge detection, colour area detection, etc) which would involve a highly complex *conventional neural network* to be able to match to other known images.

Data can be structured, unstructured, or be somewhere in between:
* Unstructured Data: There is no obvious and identifiable pattern to discern the data - a computer would have difficulty interpreting this type of data. This includes *plain text*.
* Semi-Structured Data: Content that is divided into fixed segments, such as a discernible header and body. HTML markup can be an example of this; despite it being technically plain text a computer can recognise the HTML tags and discern various different types of stored data in the document.
* *(Fully)* Structured Data: Data that is stored with known fields with fixed selection sets (e.g. months of the year), and the elements of these sets as well as the sets themselves have discernible meaning.

## Search #1: Web crawlers
A web crawler, or *spider*, is an automated bot that systematically browses the Internet, for the purpose of indexing web pages. Search engines use these crawlers to periodically update their web content, as the crawler can completely copy all the pages that it visits for later processing by the search engine.

Crawlers can be controversial - they consume server resources by initiating HTTP requests to websites, and they can scrape website data without prior approval.
* A common way to stop a web crawler from indexing site content or specific files on the web server is to declare a `robots.txt` file.

Computational efficiency can be a big problem with web crawlers, as the most common crawling search techniques are:
* Breadth-first search (GETs all links on one page, then moves onto another - may use large amounts of memory)
* Depth-first search (Follows first link of page; may never return to previous pages to load other branches of the web)

## Search #2: Indexing Engine
After texts are retrieved by the web crawler, they are passed for indexing so ultimately they can be stored in the Page Repository.

There are many different techniques to index a text:
* Full-text indexing (use all of the text)
* Full-text indexing with stopword removal
* Human-based indexing *(manual)*
* Vector Space Model of similarity
* Google PageRank

### Full-text indexing
There are several steps of full-text indexing:
* Tokenisation - break text down into words, no punctuation
* Language detection, based on characters used, and then also vocabulary and grammar
* Case conversion/normalisation
* Additional handling logic for numbers, hyphens and abbreviations (vector space model is useful here)

The results of this processing can be placed into an efficient data structure, such as a **Bag of Words**:
* Word-separated text is placed into a list
* List may be in any order
* Can eliminate linguistic context and semantics

A **bag of words** is a special case of an **N-gram model**, specifically where n=1. An **N-gram model** may be useful to attempt to preserve linguistic context, semantics and subtextual meaning.
This can be achieved by grouping words together in the list, so that every N words is a separate entry, rather than every one word with the bag-of-words model.

### Boolean indexing
Another technique commonly used to determine if a text contains a specific set of words is to build an incidence matrix of boolean values.
This works if the matrices are used for common/high-frequency words, because they can be computed ahead of time and lookup speed would be very fast.

However, this could be a big problem for large datasets, due to the large storage space required to hold these matrices.

### The Inverted Index
**Inverted Index** = A list of documents for each word, rather than a list of words for each document.
This is commonly stored as a singly-linked list, but can be stored differently depending on implementation specifics.

To create an inverted index from a Boolean IR representation, the words have to be tokenised and mapped to the documents in which they occur.
Then, an alphabetic sort is executed across the dataset (can also sort by document ID), and reduce is invoked to obtain unique documents with frequencies of words.

IBM developed a version of the inverted index called *STAIRS*, or *Storage and Information Retrieval System*. This was an efficient data structure designed to not only search through textual data by formulating boolean expressions, but also to store proximity data of words in relation to others. The result of this was that results containing synonyms, or similar words to the input expression could be returned.

STAIRS provided good search performance by indexing every word in a document except for user-selectable *stopwords* - usually common, high-frequency words like 'the', 'and' or 'but'. Two levels of index were used; a dictionary containing one occurrence of each word, and an inverted text file storing document identification and position information for each occurrence of the word. The actual document text was stored in a third file.

### Full-text indexing with restricted vocabulary
Full-text indexing can be very consuming, depending on dataset sizes, and also a waste of time.

Generally, **53% of the words in a large collection are only used once.**

Since many words are only used once, it is likely that they all will not be a good indicator as to what the text is about. Hence, in many cases, these words can be removed to increase indexing performance.

The way that this is done is typically by:
* Limit by (characteristics of) word frequency
* Apply stoplists
* Apply stemming

#### Zipf's law
* People are generally lazy in choice of language and they repeat words often.

Zipf's law, formulated empirically by the linguist George Kingsley Zipf, states that **the frequency of any word is inversely proportional to its ranking in the frequency table** (for a large sample of words).

Furthermore, the ranking of a given word multiplied by its frequency yields a constant; this is approximately **one-tenth of the tokens in the collection**.

This law almost always applies for collections with a large token count. We can use this to try and discover systematic differences between texts or collections.

Zipf's law can be expressed by the formula
`y = kx^c`
where
> y = frequency of word<br>
> k = approximately 0.1T, where T is token count<br>
> x = rank of word<br>
> c = approximately -1

This relationship produces a straight line on a double logarithmic graph of frequency and rank - with a gradient of -1 and an intercept of `log(k)`.
