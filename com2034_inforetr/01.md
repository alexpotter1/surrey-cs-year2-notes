# Information Retrieval
## Introduction

Information Retrieval, or 'IR' for short, refers to obtaining information resources that are relevant and specific to an information requirement from a collection of sources. Searches for relevant information can be based on full-text searching, or other content-based indexing. In itself, information retrieval is a science; it is an umbrella with regards to many different methods of searching for information in a document, *sets* of documents, and even associated metadata that are linked to documents.

* Web search engines are the most commonly identifiable applications that utilise IR.

### Why is this useful?
Information Retrieval allows for systemic categorisation of resources, and for efficient categorisation of data and documents.

*Peter Morville* defined the term "Findability" in 2005 to mean:
> The ability to identify an appropriate Web site and navigate the pages of the site to discover and retrieve relevant information resources.

Findability falls under the domain of **search engine optimisation (SEO)**, **internet marketing** and **data analytics**.

There is simply too much information in existence, especially on the Internet, to be able to process in an efficient manner. In addition, with respect to social media and inter-connectivity with online services, the rate of data generation has increased exponentially over the last 10 years.

* 90% of the world's data has been generated over the last two years.

* The volume of data generated by Google's self-autonomous car is estimated to be `1GB/s` per vehicle per second.

* Eric Schmidt, of Google, Inc, said this in 2010:
  > The estimated size of all information created from the dawn of civilisation to 2003 is around 5 exabytes. At Google, we process that volume of data every 2-3 days.

 *NB: 1 exabyte = 1 million terabytes*

So, it's obvious that there needs to be huge improvements in not only network transmission capabilities, but storage and retrieval systems to be able to manage this data appropriately.

### The searchable web
Search engines, such as Google, DuckDuckGo and Bing, only cover a small proportion of the web.

* The **deep web** refers to information that is not directly indexable by web crawlers. For example, this may be private data beind a password-protected portal.
* The **deep web** is estimated to be around 400-500x the size of the **searchable web**.
* Google's page index volume has not changed significantly in recent years (~25bn pages), despite exponential growth in data generation
* Google biases results in terms of search order based on **Google PageRank**, their primary searching and categorisation algorithm, as well as website reputation and advertising campaigns.
* Different search engines will return different results, depending on the data stored in their data warehouses and specific algorithms that they use to return results.
* Page ranking can vary due to **on-page**, and **off-page** link factors.

Search algorithms and techniques do not work most efficiently at all levels of scale!
* Some are better suited to small datasets, others are suited to large datasets.

### Key metrics of Big Data
These are otherwise known as the **_4 V's of Big Data_**:
* **Volume**: Scale of data that is being manipulated.
* **Variety**: The different forms of data being manipulated.
* **Velocity**: How fast streaming data enters the system
* **Veracity**: How uncertain and unpredictable the data is

### Types of data order
This module focuses on text-based searching and retrieval of data using alphanumeric tokens. This is relatively easy, compared to image analysis (edge detection, colour area detection, etc) which would involve a highly complex *conventional neural network* to be able to match to other known images.

Data can be structured, unstructured, or be somewhere in between:
* Unstructured Data: There is no obvious and identifiable pattern to discern the data - a computer would have difficulty interpreting this type of data. This includes *plain text*.
* Semi-Structured Data: Content that is divided into fixed segments, such as a discernible header and body. HTML markup can be an example of this; despite it being technically plain text a computer can recognise the HTML tags and discern various different types of stored data in the document.
* *(Fully)* Structured Data: Data that is stored with known fields with fixed selection sets (e.g. months of the year), and the elements of these sets as well as the sets themselves have discernible meaning.

## Search #1: Web crawlers
A web crawler, or *spider*, is an automated bot that systematically browses the Internet, for the purpose of indexing web pages. Search engines use these crawlers to periodically update their web content, as the crawler can completely copy all the pages that it visits for later processing by the search engine.

Crawlers can be controversial - they consume server resources by initiating HTTP requests to websites, and they can scrape website data without prior approval.
* A common way to stop a web crawler from indexing site content or specific files on the web server is to declare a `robots.txt` file.

Computational efficiency can be a big problem with web crawlers, as the most common crawling search techniques are:
* Breadth-first search (GETs all links on one page, then moves onto another - may use large amounts of memory)
* Depth-first search (Follows first link of page; may never return to previous pages to load other branches of the web)
